{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: API Interaction with Titan Takeoff Server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hardware: Each Model is running with 2 GPUs on a 4xL4 Machine\n",
    "\n",
    "1. Llama 8B (Text Generation)\n",
    "2. InternVL (Image Reasoning Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://rag-demo:3001/reader_groups\"\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "parsed_data = json.loads(response.text)\n",
    "\n",
    "for reader, details in parsed_data.items():\n",
    "    if not \"reader\" in reader:\n",
    "        print(f\"Reader: {reader}\")\n",
    "        for detail in details:\n",
    "            print(f\"  Device: {detail['device']}\")\n",
    "            print(f\"  Model Name: {detail['model_name']}\")\n",
    "            print(f\"  Model Type: {detail['model_type']}\")\n",
    "            print(f\"  Ready: {detail['ready']}\")\n",
    "            print(\"-\" * 40)  # Separator for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(filename=\"reader.png\", width=400, height=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Example: Llama 8B (Text Generation Model)\n",
    "\n",
    "Input: Text \\\n",
    "Output: Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with Takeoff Client Generate Stream Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def generate_stream(text: Union[str, List[str]],\n",
    "                    sampling_temperature: float = None,\n",
    "                    sampling_topp: float = None,\n",
    "                    sampling_topk: int = None,\n",
    "                    repetition_penalty: float = None,\n",
    "                    no_repeat_ngram_size: int = None,\n",
    "                    max_new_tokens: int = None,\n",
    "                    min_new_tokens: int = None,\n",
    "                    regex_string: str = None,\n",
    "                    json_schema: dict = None,\n",
    "                    prompt_max_tokens: int = None,\n",
    "                    consumer_group: str = \"primary\",\n",
    "                    image_path: Optional[Path] = None) -> Iterator[Event]\n",
    "\n",
    "                    '''\n",
    "\n",
    "from takeoff_client import TakeoffClient\n",
    "\n",
    "client = TakeoffClient(base_url=\"http://rag-demo\")\n",
    "\n",
    "generator = client.generate_stream('What is Generative AI?',\n",
    "sampling_temperature=0.1, consumer_group=\"llama-8b\", max_new_tokens=250)\n",
    "for event in generator:\n",
    "    print(event.data, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Compatiable Endpoints for Seamless Integration with other frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://rag-demo:3003/v1\",\n",
    "    api_key=\"no api needed\"\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"llama-8b\", # should be consumer group in takeoff\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is Generative AI?\"},\n",
    "    ],\n",
    "    stream=False,\n",
    ")\n",
    "print(chat_completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://rag-demo:3003/v1\",\n",
    "    api_key=\"no api needed\"\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"llama-8b\", # should be consumer group in takeoff\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is Digital Pathology?\"},\n",
    "    ],\n",
    "    stream=False,\n",
    "    max_tokens=250,\n",
    ")\n",
    "print(chat_completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Example #2 : internvl-4b (Image Reasoning Model)\n",
    "\n",
    "Input: Image, Text (Prompt) \\\n",
    "Output: Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example #1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from takeoff_client import TakeoffClient\n",
    "from pathlib import Path\n",
    "\n",
    "file_path = Path(\"image1.jpg\")\n",
    "\n",
    "client = TakeoffClient(base_url=\"http://rag-demo\")\n",
    "\n",
    "generator = client.generate_stream(' <image> Describe what you see in the image',\n",
    "sampling_temperature=0.1, consumer_group=\"internvl-4b\", max_new_tokens=250, image_path=file_path)\n",
    "for event in generator:\n",
    "    print(event.data, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(filename=\"image1.jpg\", width=200, height=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example #2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(filename=\"image2.jpg\", width=200, height=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from takeoff_client import TakeoffClient\n",
    "from pathlib import Path\n",
    "\n",
    "file_path = Path(\"image2.jpg\")\n",
    "\n",
    "client = TakeoffClient(base_url=\"http://rag-demo\")\n",
    "\n",
    "generator = client.generate_stream(' <image> Describe the injury you see in the image',\n",
    "sampling_temperature=0.1, consumer_group=\"internvl-4b\", max_new_tokens=300, image_path=file_path)\n",
    "for event in generator:\n",
    "    print(event.data, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(filename=\"image3.jpg\", width=200, height=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from takeoff_client import TakeoffClient\n",
    "from pathlib import Path\n",
    "\n",
    "file_path = Path(\"image3.jpg\")\n",
    "\n",
    "client = TakeoffClient(base_url=\"http://rag-demo\")\n",
    "\n",
    "generator = client.generate_stream(' <image> Describe anything you see in the image',\n",
    "sampling_temperature=0.1, consumer_group=\"internvl-4b\", max_new_tokens=250, image_path=file_path)\n",
    "for event in generator:\n",
    "    print(event.data, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric API via Prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prometheus_api_client import PrometheusConnect\n",
    "\n",
    "prom = PrometheusConnect(url = \"http://rag-demo:9090\", disable_ssl=True)\n",
    "\n",
    "# Get the list of all the metrics that the Prometheus host scrapes\n",
    "print(f\"Total Metric Count: {len(prom.all_metrics())}\")\n",
    "prom.all_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prometheus_api_client import PrometheusConnect, MetricRangeDataFrame\n",
    "import matplotlib as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "prom = PrometheusConnect(url = \"http://rag-demo:9090\", disable_ssl=True)\n",
    "\n",
    "end_time = datetime.now()\n",
    "start_time = end_time - timedelta(days=1)\n",
    "\n",
    "metric_data = prom.get_metric_range_data(\n",
    "    metric_name=\"http_requests_total\",\n",
    "    start_time=start_time,\n",
    "    end_time=end_time,\n",
    ")\n",
    "\n",
    "df = MetricRangeDataFrame(metric_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Filter the DataFrame for relevant paths\n",
    "generate_paths = df[df['path'].str.contains('/generate', case=False)]\n",
    "image_generate_paths = df[df['path'].str.contains('/image_generate', case=False)]\n",
    "\n",
    "# Step 2: Group by time or another relevant column (if available)\n",
    "# Assuming you have a 'timestamp' column, group by it and sum the counts\n",
    "generate_grouped = generate_paths.groupby('timestamp')['value'].sum()\n",
    "image_generate_grouped = image_generate_paths.groupby('timestamp')['value'].sum()\n",
    "\n",
    "# Step 3: Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the generate data\n",
    "plt.plot(generate_grouped.index, generate_grouped.values, label='Generate', color='blue')\n",
    "\n",
    "# Plot the image_generate data\n",
    "plt.plot(image_generate_grouped.index, image_generate_grouped.values, label='Image Generate', color='green')\n",
    "\n",
    "# Step 4: Add labels, title, and legend\n",
    "plt.title(\"Generate vs Image Generate Requests Over Time\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Request Count\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
